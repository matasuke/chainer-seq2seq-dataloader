{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from Seq2SeqDataset import Seq2SeqDatasetBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset paths\n",
    "source_train_path = 'data/ja_dataset_train.pkl'\n",
    "target_train_path = 'data/en_dataset_train.pkl'\n",
    "source_vocab_path = 'data/ja_vocab.pkl'\n",
    "target_vocab_path = 'data/en_vocab.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set min and max tokens to be emitte.\n",
    "n_source_min_tokens = 1\n",
    "n_source_max_tokens = 50\n",
    "n_target_min_tokens = 1\n",
    "n_target_max_tokens = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset using chainer.dataset.Mixin wrapper.\n",
    "train_data = Seq2SeqDatasetBase(\n",
    "                            source_train_path,\n",
    "                            target_train_path,\n",
    "                            source_vocab_path,\n",
    "                            target_vocab_path,\n",
    "                            n_source_min_tokens,\n",
    "                            n_source_max_tokens,\n",
    "                            n_target_min_tokens,\n",
    "                            n_target_max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1, 282,   8, 693,  12, 694,   5,  32,  18,   9,   0,  50,  52,\n",
       "          6,   2], dtype=int32),\n",
       " array([  1, 131, 798,  16,  67,   0,  48, 706,   2], dtype=int32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoded sentences can be load just by calling get_example()\n",
    "train_data.get_example(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decodec sentences\n",
      "['<SOS>', '多く', 'の', '動物', 'が', '人間', 'に', 'よ', 'っ', 'て', '<UNK>', 'さ', 'れ', 'た', '<EOS>']\n",
      "['<SOS>', 'many', 'animals', 'have', 'been', '<UNK>', 'by', 'men', '<EOS>']\n",
      "encoded sentences\n",
      "[1, 282, 8, 693, 12, 694, 5, 32, 18, 9, 0, 50, 52, 6, 2]\n",
      "[1, 131, 798, 16, 67, 0, 48, 706, 2]\n"
     ]
    }
   ],
   "source": [
    "# encoded sentences fetched by get_example() can be decoded by source_index2token() and target_index2token().\n",
    "\n",
    "enc_source_sentence, enc_target_sentence = train_data.get_example(1)\n",
    "\n",
    "dec_source_sentence = train_data.source_index2token(enc_source_sentence)\n",
    "dec_target_sentence = train_data.target_index2token(enc_target_sentence)\n",
    "\n",
    "print('decodec sentences')\n",
    "print(dec_source_sentence)\n",
    "print(dec_target_sentence)\n",
    "\n",
    "# tokens also can be encoded by source_token2index() and target_token2index().\n",
    "re_enc_source_sentence = train_data.source_token2index(dec_source_sentence)\n",
    "re_enc_target_sentence = train_data.target_token2index(dec_target_sentence)\n",
    "\n",
    "print('encoded sentences')\n",
    "print(re_enc_source_sentence)\n",
    "print(re_enc_target_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get list of words by get_source_word_ids and get_target_word_ids.\n",
    "source_word_ids = train_data.get_source_word_ids\n",
    "target_word_ids = train_data.get_target_word_ids\n",
    "\n",
    "#print(source_word_ids)\n",
    "#print(target_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of source words: 2635\n",
      "The number of target words: 2367\n"
     ]
    }
   ],
   "source": [
    "# number of words\n",
    "num_source_words = len(source_word_ids)\n",
    "num_target_words = len(target_word_ids)\n",
    "\n",
    "print('The number of source words: %d' % num_source_words)\n",
    "print('The number of target words: %d' % num_target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source unk ratio: 1.593 percent\n",
      "target unk ratio: 1.619 percent\n"
     ]
    }
   ],
   "source": [
    "# you can get <UNK> ratio by source_unk_ratio and target_unk_ratio.\n",
    "source_unk_ratio = train_data.source_unk_ratio\n",
    "target_unk_ratio = train_data.target_unk_ratio\n",
    "\n",
    "print('source unk ratio: %.3f percent' % source_unk_ratio)\n",
    "print('target unk ratio: %.3f percent' % target_unk_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you just need to send train_data to chainer.iterators.SerialIterator()\n",
    "batchsize = 128\n",
    "train_iter = chainer.iterators.SerialIterator(train_data, batchsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
